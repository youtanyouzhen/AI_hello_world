{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 5000\n",
    "VAL_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 导入mnist\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分训练集和测试集\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 查看训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据类型\n",
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 查看测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作图查看mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "digits = np.concatenate(train_images[:10], axis=1) \n",
    "plt.imshow(digits)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从训练集中提取出验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images, val_labels = train_images[:VAL_SIZE], train_labels[:VAL_SIZE]\n",
    "new_train_images, new_train_labels = train_images[-TRAIN_SIZE:], train_labels[-TRAIN_SIZE:]\n",
    "new_train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入输出转成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  输入的28*28图像转成向量  用flatten\n",
    "train_vectors = new_train_images.reshape((TRAIN_SIZE, 28 * 28))\n",
    "test_vectors = test_images.reshape((10000, 28 * 28))\n",
    "val_vectors = val_images.reshape((VAL_SIZE, 28 * 28))\n",
    "\n",
    "# 检查训练集输入\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# 将labels转为one-hot向量\n",
    "train_one_hots = to_categorical(new_train_labels)\n",
    "test_one_hots = to_categorical(test_labels)\n",
    "val_one_hots = to_categorical(val_labels)\n",
    "\n",
    "# 检查训练集输出\n",
    "train_one_hots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化前： [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  38 237 220  11   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "\n",
      "归一化后： [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.14901961 0.92941177 0.8627451  0.04313726\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('归一化前：', train_vectors[0][110:150])\n",
    "print()\n",
    "\n",
    "train_vectors_norm = train_vectors.astype('float32') / 255\n",
    "test_vectors_norm = test_vectors.astype('float32') / 255\n",
    "val_vectors_norm = val_vectors.astype('float32') / 255\n",
    "\n",
    "print('归一化后：', train_vectors_norm[0][110:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 271,986\n",
      "Trainable params: 265,438\n",
      "Non-trainable params: 6,548\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 导入\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "# 搭建网路\n",
    "network = models.Sequential()\n",
    "kernel_regularizer =  None #regularizers.l2(1)\n",
    "kernel_initializer= 'glorot_uniform'\n",
    "activation =None#layers.LeakyReLU(alpha=0.3)\n",
    "activation_layer = layers.LeakyReLU(alpha=0.3) #layers.Activation('relu')\n",
    "\n",
    "# 第一层\n",
    "network.add(layers.Dense(64, activation=activation, kernel_regularizer=kernel_regularizer, kernel_initializer=kernel_initializer, input_shape=(28 * 28,))) # relu\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(activation_layer)\n",
    "\n",
    "for i in range(50):\n",
    "    network.add(layers.Dense(64, activation=activation , kernel_regularizer=kernel_regularizer, kernel_initializer=kernel_initializer)) # relu, sigmoid\n",
    "    network.add(layers.BatchNormalization())\n",
    "    network.add(activation_layer)\n",
    "    \n",
    "network.add(layers.Dense(10, kernel_regularizer=kernel_regularizer, kernel_initializer=kernel_initializer))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add( layers.Activation('softmax'))\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编译网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "network.compile(optimizer=optimizers.adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy']) #mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 26s 5ms/step - loss: 2.5287 - accuracy: 0.1022 - val_loss: 2.3026 - val_accuracy: 0.0870\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 1s 258us/step - loss: 2.3347 - accuracy: 0.1092 - val_loss: 2.2896 - val_accuracy: 0.1020\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 2.3103 - accuracy: 0.1054 - val_loss: 2.2999 - val_accuracy: 0.1180\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 1s 256us/step - loss: 2.3046 - accuracy: 0.1038 - val_loss: 2.3006 - val_accuracy: 0.1160\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 1s 259us/step - loss: 2.3040 - accuracy: 0.1130 - val_loss: 2.3007 - val_accuracy: 0.1170\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 1s 246us/step - loss: 2.3040 - accuracy: 0.1152 - val_loss: 2.3004 - val_accuracy: 0.1170\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.3044 - accuracy: 0.1092 - val_loss: 2.3007 - val_accuracy: 0.1170\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 1s 245us/step - loss: 2.3044 - accuracy: 0.1166 - val_loss: 2.3004 - val_accuracy: 0.1170\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 2.3021 - accuracy: 0.1172 - val_loss: 2.3007 - val_accuracy: 0.1170\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.2972 - accuracy: 0.1192 - val_loss: 2.3017 - val_accuracy: 0.1170\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.2909 - accuracy: 0.1276 - val_loss: 2.3015 - val_accuracy: 0.1170\n",
      "Epoch 12/200\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 2.2896 - accuracy: 0.1262 - val_loss: 2.3020 - val_accuracy: 0.1170\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 1s 250us/step - loss: 2.2705 - accuracy: 0.1390 - val_loss: 2.2142 - val_accuracy: 0.1750\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 2.2552 - accuracy: 0.1396 - val_loss: 2.1103 - val_accuracy: 0.1650\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 1s 246us/step - loss: 2.2415 - accuracy: 0.1432 - val_loss: 2.1299 - val_accuracy: 0.1950\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 2.2356 - accuracy: 0.1438 - val_loss: 2.2130 - val_accuracy: 0.1420\n",
      "Epoch 17/200\n",
      "5000/5000 [==============================] - 1s 283us/step - loss: 2.2245 - accuracy: 0.1512 - val_loss: 2.2200 - val_accuracy: 0.1420\n",
      "Epoch 18/200\n",
      "5000/5000 [==============================] - 1s 244us/step - loss: 2.2244 - accuracy: 0.1502 - val_loss: 2.2163 - val_accuracy: 0.1760\n",
      "Epoch 19/200\n",
      "5000/5000 [==============================] - 1s 247us/step - loss: 2.2245 - accuracy: 0.1446 - val_loss: 2.2375 - val_accuracy: 0.1600\n",
      "Epoch 20/200\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 2.2168 - accuracy: 0.1442 - val_loss: 2.2590 - val_accuracy: 0.1510\n",
      "Epoch 21/200\n",
      "5000/5000 [==============================] - 1s 243us/step - loss: 2.2139 - accuracy: 0.1480 - val_loss: 2.2720 - val_accuracy: 0.1420\n",
      "Epoch 22/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.2073 - accuracy: 0.1538 - val_loss: 2.2703 - val_accuracy: 0.1500\n",
      "Epoch 23/200\n",
      "5000/5000 [==============================] - 1s 282us/step - loss: 2.2123 - accuracy: 0.1504 - val_loss: 2.2201 - val_accuracy: 0.1710\n",
      "Epoch 24/200\n",
      "5000/5000 [==============================] - 1s 288us/step - loss: 2.2081 - accuracy: 0.1528 - val_loss: 2.1615 - val_accuracy: 0.1930\n",
      "Epoch 25/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.2155 - accuracy: 0.1528 - val_loss: 2.1398 - val_accuracy: 0.1970\n",
      "Epoch 26/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.2115 - accuracy: 0.1504 - val_loss: 2.1188 - val_accuracy: 0.1900\n",
      "Epoch 27/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.2031 - accuracy: 0.1564 - val_loss: 2.1693 - val_accuracy: 0.1830\n",
      "Epoch 28/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.2120 - accuracy: 0.1500 - val_loss: 2.2364 - val_accuracy: 0.1580\n",
      "Epoch 29/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.2043 - accuracy: 0.1524 - val_loss: 2.2887 - val_accuracy: 0.1270\n",
      "Epoch 30/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.2014 - accuracy: 0.1506 - val_loss: 2.2480 - val_accuracy: 0.1640\n",
      "Epoch 31/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1884 - accuracy: 0.1556 - val_loss: 2.3400 - val_accuracy: 0.1170\n",
      "Epoch 32/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1921 - accuracy: 0.1564 - val_loss: 2.3223 - val_accuracy: 0.1240\n",
      "Epoch 33/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.1889 - accuracy: 0.1588 - val_loss: 2.2168 - val_accuracy: 0.1820\n",
      "Epoch 34/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.1746 - accuracy: 0.1630 - val_loss: 2.2457 - val_accuracy: 0.1710\n",
      "Epoch 35/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1686 - accuracy: 0.1642 - val_loss: 2.1454 - val_accuracy: 0.2010\n",
      "Epoch 36/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.1783 - accuracy: 0.1640 - val_loss: 2.0468 - val_accuracy: 0.2060\n",
      "Epoch 37/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1740 - accuracy: 0.1660 - val_loss: 2.0177 - val_accuracy: 0.2120\n",
      "Epoch 38/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.1622 - accuracy: 0.1698 - val_loss: 2.0146 - val_accuracy: 0.2220\n",
      "Epoch 39/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1607 - accuracy: 0.1694 - val_loss: 2.0119 - val_accuracy: 0.2080\n",
      "Epoch 40/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.1450 - accuracy: 0.1764 - val_loss: 2.0089 - val_accuracy: 0.2080\n",
      "Epoch 41/200\n",
      "5000/5000 [==============================] - 1s 251us/step - loss: 2.1453 - accuracy: 0.1752 - val_loss: 2.0093 - val_accuracy: 0.2080\n",
      "Epoch 42/200\n",
      "5000/5000 [==============================] - 1s 276us/step - loss: 2.1504 - accuracy: 0.1724 - val_loss: 2.0709 - val_accuracy: 0.2070\n",
      "Epoch 43/200\n",
      "5000/5000 [==============================] - 1s 247us/step - loss: 2.1409 - accuracy: 0.1632 - val_loss: 2.0422 - val_accuracy: 0.2070\n",
      "Epoch 44/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.1122 - accuracy: 0.1784 - val_loss: 1.9937 - val_accuracy: 0.2070\n",
      "Epoch 45/200\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 2.1033 - accuracy: 0.1826 - val_loss: 2.0396 - val_accuracy: 0.2060\n",
      "Epoch 46/200\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 2.0944 - accuracy: 0.1790 - val_loss: 2.0328 - val_accuracy: 0.2060\n",
      "Epoch 47/200\n",
      "5000/5000 [==============================] - 1s 244us/step - loss: 2.0839 - accuracy: 0.1826 - val_loss: 2.0121 - val_accuracy: 0.2050\n",
      "Epoch 48/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.1642 - accuracy: 0.1812 - val_loss: 2.1834 - val_accuracy: 0.2000\n",
      "Epoch 49/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.1252 - accuracy: 0.1824 - val_loss: 2.8348 - val_accuracy: 0.2040\n",
      "Epoch 50/200\n",
      "5000/5000 [==============================] - 1s 243us/step - loss: 2.1016 - accuracy: 0.1820 - val_loss: 2.1277 - val_accuracy: 0.2070\n",
      "Epoch 51/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.0924 - accuracy: 0.1844 - val_loss: 3.1030 - val_accuracy: 0.2070\n",
      "Epoch 52/200\n",
      "5000/5000 [==============================] - 1s 243us/step - loss: 2.0743 - accuracy: 0.1876 - val_loss: 2.1674 - val_accuracy: 0.2060\n",
      "Epoch 53/200\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 2.1617 - accuracy: 0.1786 - val_loss: 3.0213 - val_accuracy: 0.2050\n",
      "Epoch 54/200\n",
      "5000/5000 [==============================] - 1s 247us/step - loss: 2.1130 - accuracy: 0.1842 - val_loss: 10.8219 - val_accuracy: 0.2060\n",
      "Epoch 55/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.0743 - accuracy: 0.1908 - val_loss: 3.8636 - val_accuracy: 0.2080\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 245us/step - loss: 2.0533 - accuracy: 0.1946 - val_loss: 4.8595 - val_accuracy: 0.2100\n",
      "Epoch 57/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.0926 - accuracy: 0.1950 - val_loss: 2.8581 - val_accuracy: 0.2090\n",
      "Epoch 58/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.0825 - accuracy: 0.1918 - val_loss: 2.3279 - val_accuracy: 0.2100\n",
      "Epoch 59/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1468 - accuracy: 0.1852 - val_loss: 2.1635 - val_accuracy: 0.1980\n",
      "Epoch 60/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1839 - accuracy: 0.1702 - val_loss: 2.0393 - val_accuracy: 0.1860\n",
      "Epoch 61/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.2540 - accuracy: 0.1554 - val_loss: 2.2436 - val_accuracy: 0.2000\n",
      "Epoch 62/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.2831 - accuracy: 0.1498 - val_loss: 2.0930 - val_accuracy: 0.2080\n",
      "Epoch 63/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.2262 - accuracy: 0.1594 - val_loss: 2.0534 - val_accuracy: 0.2080\n",
      "Epoch 64/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.1921 - accuracy: 0.1792 - val_loss: 2.1146 - val_accuracy: 0.2090\n",
      "Epoch 65/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1752 - accuracy: 0.1736 - val_loss: 2.1457 - val_accuracy: 0.2090\n",
      "Epoch 66/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.1529 - accuracy: 0.1814 - val_loss: 2.0545 - val_accuracy: 0.2080\n",
      "Epoch 67/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1442 - accuracy: 0.1862 - val_loss: 2.1353 - val_accuracy: 0.2080\n",
      "Epoch 68/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1305 - accuracy: 0.1888 - val_loss: 2.1089 - val_accuracy: 0.2070\n",
      "Epoch 69/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.1151 - accuracy: 0.1848 - val_loss: 2.0501 - val_accuracy: 0.2080\n",
      "Epoch 70/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.1134 - accuracy: 0.1894 - val_loss: 2.0184 - val_accuracy: 0.2090\n",
      "Epoch 71/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.1003 - accuracy: 0.1938 - val_loss: 1.9993 - val_accuracy: 0.2090\n",
      "Epoch 72/200\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 2.1103 - accuracy: 0.1954 - val_loss: 2.2274 - val_accuracy: 0.1880\n",
      "Epoch 73/200\n",
      "5000/5000 [==============================] - 2s 336us/step - loss: 2.2023 - accuracy: 0.1780 - val_loss: 2.1847 - val_accuracy: 0.1670\n",
      "Epoch 74/200\n",
      "5000/5000 [==============================] - 1s 287us/step - loss: 2.1496 - accuracy: 0.1746 - val_loss: 2.1326 - val_accuracy: 0.1930\n",
      "Epoch 75/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.2229 - accuracy: 0.1604 - val_loss: 2.4317 - val_accuracy: 0.1050\n",
      "Epoch 76/200\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 2.2870 - accuracy: 0.1488 - val_loss: 2.1755 - val_accuracy: 0.1470\n",
      "Epoch 77/200\n",
      "5000/5000 [==============================] - 1s 255us/step - loss: 2.2790 - accuracy: 0.1490 - val_loss: 2.2295 - val_accuracy: 0.1840\n",
      "Epoch 78/200\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 2.2574 - accuracy: 0.1538 - val_loss: 2.1827 - val_accuracy: 0.1860\n",
      "Epoch 79/200\n",
      "5000/5000 [==============================] - 1s 271us/step - loss: 2.2599 - accuracy: 0.1512 - val_loss: 2.1996 - val_accuracy: 0.1840\n",
      "Epoch 80/200\n",
      "5000/5000 [==============================] - 1s 247us/step - loss: 2.2596 - accuracy: 0.1528 - val_loss: 2.1914 - val_accuracy: 0.1370\n",
      "Epoch 81/200\n",
      "5000/5000 [==============================] - 1s 250us/step - loss: 2.2468 - accuracy: 0.1534 - val_loss: 2.1768 - val_accuracy: 0.1740\n",
      "Epoch 82/200\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 2.2325 - accuracy: 0.1576 - val_loss: 2.1597 - val_accuracy: 0.1760\n",
      "Epoch 83/200\n",
      "5000/5000 [==============================] - 1s 251us/step - loss: 2.2116 - accuracy: 0.1608 - val_loss: 2.1170 - val_accuracy: 0.2020\n",
      "Epoch 84/200\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 2.1837 - accuracy: 0.1690 - val_loss: 2.0952 - val_accuracy: 0.2040\n",
      "Epoch 85/200\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 2.1667 - accuracy: 0.1722 - val_loss: 2.0549 - val_accuracy: 0.2120\n",
      "Epoch 86/200\n",
      "5000/5000 [==============================] - 1s 265us/step - loss: 2.1521 - accuracy: 0.1798 - val_loss: 2.0028 - val_accuracy: 0.2170\n",
      "Epoch 87/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.1307 - accuracy: 0.1868 - val_loss: 1.9862 - val_accuracy: 0.2160\n",
      "Epoch 88/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.1105 - accuracy: 0.1838 - val_loss: 1.9826 - val_accuracy: 0.2150\n",
      "Epoch 89/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.0982 - accuracy: 0.1900 - val_loss: 1.9913 - val_accuracy: 0.1960\n",
      "Epoch 90/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.0938 - accuracy: 0.1814 - val_loss: 2.0378 - val_accuracy: 0.2070\n",
      "Epoch 91/200\n",
      "5000/5000 [==============================] - 1s 257us/step - loss: 2.1235 - accuracy: 0.1726 - val_loss: 2.1178 - val_accuracy: 0.2110\n",
      "Epoch 92/200\n",
      "5000/5000 [==============================] - 1s 244us/step - loss: 2.1384 - accuracy: 0.1730 - val_loss: 2.0923 - val_accuracy: 0.2100\n",
      "Epoch 93/200\n",
      "5000/5000 [==============================] - 1s 249us/step - loss: 2.1296 - accuracy: 0.1776 - val_loss: 2.0137 - val_accuracy: 0.2120\n",
      "Epoch 94/200\n",
      "5000/5000 [==============================] - 1s 249us/step - loss: 2.1233 - accuracy: 0.1734 - val_loss: 2.0119 - val_accuracy: 0.2120\n",
      "Epoch 95/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.1123 - accuracy: 0.1846 - val_loss: 2.0448 - val_accuracy: 0.2110\n",
      "Epoch 96/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.0899 - accuracy: 0.1848 - val_loss: 2.0890 - val_accuracy: 0.2110\n",
      "Epoch 97/200\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 2.0876 - accuracy: 0.1858 - val_loss: 2.0379 - val_accuracy: 0.2120\n",
      "Epoch 98/200\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 2.0669 - accuracy: 0.1914 - val_loss: 2.0331 - val_accuracy: 0.2100\n",
      "Epoch 99/200\n",
      "5000/5000 [==============================] - 1s 251us/step - loss: 2.0566 - accuracy: 0.1888 - val_loss: 2.0021 - val_accuracy: 0.2120\n",
      "Epoch 100/200\n",
      "5000/5000 [==============================] - 1s 252us/step - loss: 2.1304 - accuracy: 0.1728 - val_loss: 2.0056 - val_accuracy: 0.2130\n",
      "Epoch 101/200\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 2.2143 - accuracy: 0.1616 - val_loss: 2.0686 - val_accuracy: 0.2080\n",
      "Epoch 102/200\n",
      "5000/5000 [==============================] - 1s 253us/step - loss: 2.1790 - accuracy: 0.1658 - val_loss: 2.0342 - val_accuracy: 0.2100\n",
      "Epoch 103/200\n",
      "5000/5000 [==============================] - 1s 254us/step - loss: 2.1922 - accuracy: 0.1598 - val_loss: 2.0345 - val_accuracy: 0.2100\n",
      "Epoch 104/200\n",
      "5000/5000 [==============================] - 1s 258us/step - loss: 2.1594 - accuracy: 0.1730 - val_loss: 2.0276 - val_accuracy: 0.2100\n",
      "Epoch 105/200\n",
      "5000/5000 [==============================] - 1s 248us/step - loss: 2.1385 - accuracy: 0.1798 - val_loss: 2.0173 - val_accuracy: 0.2100\n",
      "Epoch 106/200\n",
      "5000/5000 [==============================] - 1s 246us/step - loss: 2.1322 - accuracy: 0.1716 - val_loss: 2.0066 - val_accuracy: 0.2100\n",
      "Epoch 107/200\n",
      "5000/5000 [==============================] - 1s 243us/step - loss: 2.1161 - accuracy: 0.1814 - val_loss: 1.9904 - val_accuracy: 0.2110\n",
      "Epoch 108/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.2681 - accuracy: 0.1468 - val_loss: 2.0319 - val_accuracy: 0.2130\n",
      "Epoch 109/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.2251 - accuracy: 0.1564 - val_loss: 2.0708 - val_accuracy: 0.2120\n",
      "Epoch 110/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.2056 - accuracy: 0.1628 - val_loss: 2.0655 - val_accuracy: 0.2030\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.2104 - accuracy: 0.1524 - val_loss: 2.0960 - val_accuracy: 0.2170\n",
      "Epoch 112/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2952 - accuracy: 0.1366 - val_loss: 2.1289 - val_accuracy: 0.2120\n",
      "Epoch 113/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.3007 - accuracy: 0.1338 - val_loss: 2.1596 - val_accuracy: 0.2150\n",
      "Epoch 114/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.2736 - accuracy: 0.1384 - val_loss: 2.1694 - val_accuracy: 0.2140\n",
      "Epoch 115/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2725 - accuracy: 0.1392 - val_loss: 2.1655 - val_accuracy: 0.2140\n",
      "Epoch 116/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2625 - accuracy: 0.1404 - val_loss: 2.1616 - val_accuracy: 0.2150\n",
      "Epoch 117/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2577 - accuracy: 0.1466 - val_loss: 2.1555 - val_accuracy: 0.2130\n",
      "Epoch 118/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.2547 - accuracy: 0.1500 - val_loss: 2.1562 - val_accuracy: 0.2130\n",
      "Epoch 119/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.2526 - accuracy: 0.1508 - val_loss: 2.1512 - val_accuracy: 0.2100\n",
      "Epoch 120/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.2356 - accuracy: 0.1522 - val_loss: 2.1467 - val_accuracy: 0.2110\n",
      "Epoch 121/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.2402 - accuracy: 0.1496 - val_loss: 2.1477 - val_accuracy: 0.2110\n",
      "Epoch 122/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2344 - accuracy: 0.1566 - val_loss: 2.1448 - val_accuracy: 0.2110\n",
      "Epoch 123/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.2194 - accuracy: 0.1652 - val_loss: 2.1317 - val_accuracy: 0.2110\n",
      "Epoch 124/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.2207 - accuracy: 0.1596 - val_loss: 2.1288 - val_accuracy: 0.2120\n",
      "Epoch 125/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.2330 - accuracy: 0.1516 - val_loss: 2.1341 - val_accuracy: 0.2130\n",
      "Epoch 126/200\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 2.2355 - accuracy: 0.1550 - val_loss: 2.1330 - val_accuracy: 0.2140\n",
      "Epoch 127/200\n",
      "5000/5000 [==============================] - 1s 262us/step - loss: 2.2200 - accuracy: 0.1558 - val_loss: 2.1342 - val_accuracy: 0.2140\n",
      "Epoch 128/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2112 - accuracy: 0.1602 - val_loss: 2.1312 - val_accuracy: 0.2130\n",
      "Epoch 129/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2062 - accuracy: 0.1632 - val_loss: 2.1257 - val_accuracy: 0.2130\n",
      "Epoch 130/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.2051 - accuracy: 0.1678 - val_loss: 2.1177 - val_accuracy: 0.2150\n",
      "Epoch 131/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2000 - accuracy: 0.1706 - val_loss: 2.1149 - val_accuracy: 0.2150\n",
      "Epoch 132/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2065 - accuracy: 0.1628 - val_loss: 2.1098 - val_accuracy: 0.2150\n",
      "Epoch 133/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2073 - accuracy: 0.1652 - val_loss: 2.1169 - val_accuracy: 0.2160\n",
      "Epoch 134/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1895 - accuracy: 0.1680 - val_loss: 2.1155 - val_accuracy: 0.2160\n",
      "Epoch 135/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1835 - accuracy: 0.1706 - val_loss: 2.1144 - val_accuracy: 0.2160\n",
      "Epoch 136/200\n",
      "5000/5000 [==============================] - 1s 239us/step - loss: 2.1677 - accuracy: 0.1752 - val_loss: 2.1083 - val_accuracy: 0.2170\n",
      "Epoch 137/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1605 - accuracy: 0.1748 - val_loss: 2.0986 - val_accuracy: 0.2170\n",
      "Epoch 138/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1632 - accuracy: 0.1806 - val_loss: 2.0854 - val_accuracy: 0.2180\n",
      "Epoch 139/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1560 - accuracy: 0.1760 - val_loss: 2.0854 - val_accuracy: 0.2170\n",
      "Epoch 140/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1499 - accuracy: 0.1804 - val_loss: 2.0737 - val_accuracy: 0.2170\n",
      "Epoch 141/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1610 - accuracy: 0.1790 - val_loss: 2.0778 - val_accuracy: 0.2170\n",
      "Epoch 142/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1649 - accuracy: 0.1758 - val_loss: 2.0867 - val_accuracy: 0.2160\n",
      "Epoch 143/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1835 - accuracy: 0.1756 - val_loss: 2.0842 - val_accuracy: 0.2130\n",
      "Epoch 144/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1606 - accuracy: 0.1796 - val_loss: 2.0778 - val_accuracy: 0.2120\n",
      "Epoch 145/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.1530 - accuracy: 0.1820 - val_loss: 2.0772 - val_accuracy: 0.2110\n",
      "Epoch 146/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1594 - accuracy: 0.1774 - val_loss: 2.0780 - val_accuracy: 0.2110\n",
      "Epoch 147/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1739 - accuracy: 0.1778 - val_loss: 2.0795 - val_accuracy: 0.2110\n",
      "Epoch 148/200\n",
      "5000/5000 [==============================] - 1s 238us/step - loss: 2.1729 - accuracy: 0.1782 - val_loss: 2.0780 - val_accuracy: 0.2110\n",
      "Epoch 149/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1629 - accuracy: 0.1800 - val_loss: 2.0724 - val_accuracy: 0.2120\n",
      "Epoch 150/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1519 - accuracy: 0.1830 - val_loss: 2.0637 - val_accuracy: 0.2140\n",
      "Epoch 151/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1448 - accuracy: 0.1800 - val_loss: 2.0538 - val_accuracy: 0.2150\n",
      "Epoch 152/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1427 - accuracy: 0.1810 - val_loss: 2.0586 - val_accuracy: 0.2150\n",
      "Epoch 153/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.1248 - accuracy: 0.1872 - val_loss: 2.0535 - val_accuracy: 0.2150\n",
      "Epoch 154/200\n",
      "5000/5000 [==============================] - 1s 242us/step - loss: 2.1231 - accuracy: 0.1916 - val_loss: 2.0526 - val_accuracy: 0.2130\n",
      "Epoch 155/200\n",
      "5000/5000 [==============================] - 1s 249us/step - loss: 2.1106 - accuracy: 0.1878 - val_loss: 2.0479 - val_accuracy: 0.2120\n",
      "Epoch 156/200\n",
      "5000/5000 [==============================] - 1s 261us/step - loss: 2.1047 - accuracy: 0.1908 - val_loss: 2.0483 - val_accuracy: 0.2130\n",
      "Epoch 157/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.0996 - accuracy: 0.1960 - val_loss: 2.0431 - val_accuracy: 0.2120\n",
      "Epoch 158/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.0950 - accuracy: 0.1926 - val_loss: 2.0372 - val_accuracy: 0.2130\n",
      "Epoch 159/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.0840 - accuracy: 0.1926 - val_loss: 2.0544 - val_accuracy: 0.2130\n",
      "Epoch 160/200\n",
      "5000/5000 [==============================] - 1s 240us/step - loss: 2.1020 - accuracy: 0.1888 - val_loss: 2.0385 - val_accuracy: 0.2130\n",
      "Epoch 161/200\n",
      "5000/5000 [==============================] - 1s 245us/step - loss: 2.0984 - accuracy: 0.1870 - val_loss: 2.0256 - val_accuracy: 0.2170\n",
      "Epoch 162/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.0819 - accuracy: 0.1884 - val_loss: 2.0274 - val_accuracy: 0.2180\n",
      "Epoch 163/200\n",
      "5000/5000 [==============================] - 1s 249us/step - loss: 2.0821 - accuracy: 0.1872 - val_loss: 2.0204 - val_accuracy: 0.2180\n",
      "Epoch 164/200\n",
      "5000/5000 [==============================] - 1s 245us/step - loss: 2.0608 - accuracy: 0.1944 - val_loss: 2.0112 - val_accuracy: 0.2170\n",
      "Epoch 165/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.0548 - accuracy: 0.1926 - val_loss: 2.0088 - val_accuracy: 0.2170\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0633 - accuracy: 0.1956 - val_loss: 2.0013 - val_accuracy: 0.2170\n",
      "Epoch 167/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0430 - accuracy: 0.1974 - val_loss: 1.9767 - val_accuracy: 0.2210\n",
      "Epoch 168/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0341 - accuracy: 0.1912 - val_loss: 1.9807 - val_accuracy: 0.1990\n",
      "Epoch 169/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.0313 - accuracy: 0.2004 - val_loss: 1.9650 - val_accuracy: 0.2250\n",
      "Epoch 170/200\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 2.0352 - accuracy: 0.1990 - val_loss: 1.9625 - val_accuracy: 0.2260\n",
      "Epoch 171/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0461 - accuracy: 0.1940 - val_loss: 1.9660 - val_accuracy: 0.2260\n",
      "Epoch 172/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.0292 - accuracy: 0.1950 - val_loss: 1.9653 - val_accuracy: 0.2260\n",
      "Epoch 173/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.0326 - accuracy: 0.1962 - val_loss: 1.9696 - val_accuracy: 0.2260\n",
      "Epoch 174/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.0288 - accuracy: 0.1926 - val_loss: 1.9645 - val_accuracy: 0.2110\n",
      "Epoch 175/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.0365 - accuracy: 0.1916 - val_loss: 1.9640 - val_accuracy: 0.2040\n",
      "Epoch 176/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.0255 - accuracy: 0.1948 - val_loss: 1.9606 - val_accuracy: 0.2530\n",
      "Epoch 177/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0309 - accuracy: 0.1976 - val_loss: 1.9613 - val_accuracy: 0.2260\n",
      "Epoch 178/200\n",
      "5000/5000 [==============================] - 1s 241us/step - loss: 2.0308 - accuracy: 0.1896 - val_loss: 1.9678 - val_accuracy: 0.2240\n",
      "Epoch 179/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.0418 - accuracy: 0.1868 - val_loss: 1.9651 - val_accuracy: 0.2250\n",
      "Epoch 180/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.0428 - accuracy: 0.1894 - val_loss: 1.9866 - val_accuracy: 0.2220\n",
      "Epoch 181/200\n",
      "5000/5000 [==============================] - 1s 230us/step - loss: 2.1089 - accuracy: 0.1786 - val_loss: 2.9555 - val_accuracy: 0.1960\n",
      "Epoch 182/200\n",
      "5000/5000 [==============================] - 1s 243us/step - loss: 2.3002 - accuracy: 0.1564 - val_loss: 4.3719 - val_accuracy: 0.1860\n",
      "Epoch 183/200\n",
      "5000/5000 [==============================] - 1s 263us/step - loss: 2.2195 - accuracy: 0.1604 - val_loss: 2.4713 - val_accuracy: 0.2040\n",
      "Epoch 184/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2197 - accuracy: 0.1640 - val_loss: 2.1827 - val_accuracy: 0.2080\n",
      "Epoch 185/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1984 - accuracy: 0.1664 - val_loss: 2.1101 - val_accuracy: 0.2080\n",
      "Epoch 186/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1822 - accuracy: 0.1698 - val_loss: 2.0946 - val_accuracy: 0.2070\n",
      "Epoch 187/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1776 - accuracy: 0.1708 - val_loss: 2.0845 - val_accuracy: 0.2070\n",
      "Epoch 188/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1799 - accuracy: 0.1698 - val_loss: 2.0697 - val_accuracy: 0.2030\n",
      "Epoch 189/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.1720 - accuracy: 0.1724 - val_loss: 2.0407 - val_accuracy: 0.2010\n",
      "Epoch 190/200\n",
      "5000/5000 [==============================] - 1s 236us/step - loss: 2.1662 - accuracy: 0.1736 - val_loss: 2.0180 - val_accuracy: 0.2070\n",
      "Epoch 191/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.1523 - accuracy: 0.1718 - val_loss: 2.0083 - val_accuracy: 0.2060\n",
      "Epoch 192/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1411 - accuracy: 0.1690 - val_loss: 2.0009 - val_accuracy: 0.2080\n",
      "Epoch 193/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.1396 - accuracy: 0.1756 - val_loss: 2.0119 - val_accuracy: 0.2100\n",
      "Epoch 194/200\n",
      "5000/5000 [==============================] - 1s 231us/step - loss: 2.1443 - accuracy: 0.1774 - val_loss: 2.0128 - val_accuracy: 0.2090\n",
      "Epoch 195/200\n",
      "5000/5000 [==============================] - 1s 237us/step - loss: 2.1520 - accuracy: 0.1748 - val_loss: 2.0123 - val_accuracy: 0.2100\n",
      "Epoch 196/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1421 - accuracy: 0.1772 - val_loss: 2.0278 - val_accuracy: 0.2080\n",
      "Epoch 197/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.1849 - accuracy: 0.1654 - val_loss: 2.1342 - val_accuracy: 0.1830\n",
      "Epoch 198/200\n",
      "5000/5000 [==============================] - 1s 234us/step - loss: 2.2355 - accuracy: 0.1556 - val_loss: 2.1394 - val_accuracy: 0.1830\n",
      "Epoch 199/200\n",
      "5000/5000 [==============================] - 1s 233us/step - loss: 2.2292 - accuracy: 0.1556 - val_loss: 2.1369 - val_accuracy: 0.1880\n",
      "Epoch 200/200\n",
      "5000/5000 [==============================] - 1s 232us/step - loss: 2.2205 - accuracy: 0.1488 - val_loss: 2.1227 - val_accuracy: 0.1930\n"
     ]
    }
   ],
   "source": [
    "history = network.fit(train_vectors_norm, train_one_hots, epochs=200, batch_size=256, validation_data=(val_vectors_norm, val_one_hots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出训练的中间数据：损失和准确度\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确率为：14.88%， 验证集准确率为：19.3%\n"
     ]
    }
   ],
   "source": [
    "# 查看准确率\n",
    "train_best_acc = f'{round(acc_values[-1]*100,2)}%'\n",
    "val_best_acc =  f'{round(val_acc_values[-1]*100,2)}%'\n",
    "print(f\"训练集准确率为：{train_best_acc}， 验证集准确率为：{val_best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2,1)\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# 损失\n",
    "axes[0].plot(epochs, loss_values, 'b-o', label='Training loss')\n",
    "axes[0].plot(epochs, val_loss_values, 'r-o', label='Validation loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('loss')\n",
    "\n",
    "# 准确度\n",
    "axes[1].plot(epochs, acc_values, 'b-o', label=f'Training (best: {train_best_acc})')\n",
    "axes[1].plot(epochs, val_acc_values, 'r-o', label=f'Validation (best: {val_best_acc})')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('accuary')\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 180us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_vectors_norm, test_one_hots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# network.predict(test_vectors_norm[0])\n",
    "test_vectors_norm[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
